{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfbR_vq_AEbk",
        "outputId": "02ad419e-8a99-41d3-877a-c42b69d9624a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRE2z3VRAA_N",
        "outputId": "6891a136-4a98-4d39-9b08-e3b18cc2083e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node2Vec main method.\n",
            "Graph info: DiGraph with 1403 nodes and 1504 edges\n",
            "\tInstantiate a node2vec object.\n",
            "\tFit node2vec.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tExtract embeddings and labels for the anchor nodes.\n",
            "<class 'list'>\n",
            "Tanay\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Node2Vec graph embedding method\n",
        "The source code is from the repository of the authors of the paper\n",
        "\"\"\"\n",
        "\n",
        "# Import Statements\n",
        "\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import gensim\n",
        "from joblib import Parallel, delayed\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import time\n",
        "import csv\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------------\n",
        "# Class Definition\n",
        "\n",
        "\n",
        "class Node2Vec:\n",
        "    FIRST_TRAVEL_KEY = 'first_travel_key'\n",
        "    PROBABILITIES_KEY = 'probabilities'\n",
        "    NEIGHBORS_KEY = 'neighbors'\n",
        "    WEIGHT_KEY = 'weight'\n",
        "    NUM_WALKS_KEY = 'num_walks'\n",
        "    WALK_LENGTH_KEY = 'walk_length'\n",
        "    P_KEY = 'p'\n",
        "    Q_KEY = 'q'\n",
        "\n",
        "    def __init__(self, graph: nx.Graph, dimensions: int = 128, walk_length: int = 80,\n",
        "                 num_walks: int = 10,\n",
        "                 p: float = 1,\n",
        "                 q: float = 1, weight_key: str = 'weight', workers: int = 10,\n",
        "                 sampling_strategy: dict = None,\n",
        "                 quiet: bool = True, temp_folder: str = None):\n",
        "        \"\"\"\n",
        "        Initiates the Node2Vec object, precomputes walking probabilities and generates the walks.\n",
        "\n",
        "        :param graph: Input graph\n",
        "        :param dimensions: Embedding dimensions (default: 128)\n",
        "        :param walk_length: Number of nodes in each walk (default: 80)\n",
        "        :param num_walks: Number of walks per node (default: 10)\n",
        "        :param p: Return hyper parameter (default: 1)\n",
        "        :param q: Inout parameter (default: 1)\n",
        "        :param weight_key: On weighted graphs, this is the key for the weight attribute (default: 'weight')\n",
        "        :param workers: Number of workers for parallel execution (default: 1)\n",
        "        :param sampling_strategy: Node specific sampling strategies, supports setting node specific 'q', 'p', 'num_walks' and 'walk_length'.\n",
        "        Use these keys exactly. If not set, will use the global ones which were passed on the object initialization\n",
        "        :param temp_folder: Path to folder with enough space to hold the memory map of self.d_graph (for big graphs); to be passed joblib.Parallel.temp_folder\n",
        "        \"\"\"\n",
        "\n",
        "        self.graph = graph\n",
        "        self.dimensions = dimensions\n",
        "        self.walk_length = walk_length\n",
        "        self.num_walks = num_walks\n",
        "        self.p = p\n",
        "        self.q = q\n",
        "        self.weight_key = weight_key\n",
        "        self.workers = workers\n",
        "        self.quiet = quiet\n",
        "        self.d_graph = defaultdict(dict)\n",
        "\n",
        "        if sampling_strategy is None:\n",
        "            self.sampling_strategy = {}\n",
        "        else:\n",
        "            self.sampling_strategy = sampling_strategy\n",
        "\n",
        "        self.temp_folder, self.require = None, None\n",
        "        if temp_folder:\n",
        "            if not os.path.isdir(temp_folder):\n",
        "                raise NotADirectoryError(\"temp_folder does not exist or is not a directory. ({})\".format(temp_folder))\n",
        "\n",
        "            self.temp_folder = temp_folder\n",
        "            self.require = \"sharedmem\"\n",
        "\n",
        "        self._precompute_probabilities()\n",
        "        self.walks = self._generate_walks()\n",
        "\n",
        "    def _precompute_probabilities(self):\n",
        "        \"\"\"\n",
        "        Precomputes transition probabilities for each node.\n",
        "        \"\"\"\n",
        "\n",
        "        d_graph = self.d_graph\n",
        "\n",
        "        # nodes_generator = self.graph.nodes() if self.quiet \\\n",
        "        #     else tqdm(self.graph.nodes(), desc='Computing transition probabilities')\n",
        "        nodes_generator = self.graph.nodes()\n",
        "\n",
        "        for source in nodes_generator:\n",
        "\n",
        "            # Init probabilities dict for first travel\n",
        "            if self.PROBABILITIES_KEY not in d_graph[source]:\n",
        "                d_graph[source][self.PROBABILITIES_KEY] = dict()\n",
        "\n",
        "            for current_node in self.graph.neighbors(source):\n",
        "\n",
        "                # Init probabilities dict\n",
        "                if self.PROBABILITIES_KEY not in d_graph[current_node]:\n",
        "                    d_graph[current_node][self.PROBABILITIES_KEY] = dict()\n",
        "\n",
        "                unnormalized_weights = list()\n",
        "                d_neighbors = list()\n",
        "\n",
        "                # Calculate unnormalized weights\n",
        "                for destination in self.graph.neighbors(current_node):\n",
        "\n",
        "                    p = self.sampling_strategy[current_node].get(self.P_KEY,\n",
        "                                                                 self.p) if current_node in self.sampling_strategy else self.p\n",
        "                    q = self.sampling_strategy[current_node].get(self.Q_KEY,\n",
        "                                                                 self.q) if current_node in self.sampling_strategy else self.q\n",
        "\n",
        "                    if destination == source:  # Backwards probability\n",
        "                        ss_weight = self.graph[current_node][destination].get(self.weight_key, 1) * 1 / p\n",
        "                    elif destination in self.graph[source]:  # If the neighbor is connected to the source\n",
        "                        ss_weight = self.graph[current_node][destination].get(self.weight_key, 1)\n",
        "                    else:\n",
        "                        ss_weight = self.graph[current_node][destination].get(self.weight_key, 1) * 1 / q\n",
        "\n",
        "                    # Assign the unnormalized sampling strategy weight, normalize during random walk\n",
        "                    unnormalized_weights.append(ss_weight)\n",
        "                    d_neighbors.append(destination)\n",
        "\n",
        "                # Normalize\n",
        "                unnormalized_weights = np.array(unnormalized_weights)\n",
        "                d_graph[current_node][self.PROBABILITIES_KEY][\n",
        "                    source] = unnormalized_weights / unnormalized_weights.sum()\n",
        "\n",
        "                # Save neighbors\n",
        "                d_graph[current_node][self.NEIGHBORS_KEY] = d_neighbors\n",
        "\n",
        "            # Calculate first_travel weights for source\n",
        "            first_travel_weights = []\n",
        "\n",
        "            for destination in self.graph.neighbors(source):\n",
        "                first_travel_weights.append(self.graph[source][destination].get(self.weight_key, 1))\n",
        "\n",
        "            first_travel_weights = np.array(first_travel_weights)\n",
        "            d_graph[source][self.FIRST_TRAVEL_KEY] = first_travel_weights / first_travel_weights.sum()\n",
        "\n",
        "    def _generate_walks(self) -> list:\n",
        "        \"\"\"\n",
        "        Generates the random walks which will be used as the skip-gram input.\n",
        "        :return: List of walks. Each walk is a list of nodes.\n",
        "        \"\"\"\n",
        "\n",
        "        flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "\n",
        "        # Split num_walks for each worker\n",
        "        num_walks_lists = np.array_split(range(self.num_walks), self.workers)\n",
        "\n",
        "        walk_results = Parallel(n_jobs=self.workers, temp_folder=self.temp_folder, require=self.require)(\n",
        "            delayed(parallel_generate_walks)(self.d_graph,\n",
        "                                             self.walk_length,\n",
        "                                             len(num_walks),\n",
        "                                             idx,\n",
        "                                             self.sampling_strategy,\n",
        "                                             self.NUM_WALKS_KEY,\n",
        "                                             self.WALK_LENGTH_KEY,\n",
        "                                             self.NEIGHBORS_KEY,\n",
        "                                             self.PROBABILITIES_KEY,\n",
        "                                             self.FIRST_TRAVEL_KEY,\n",
        "                                             self.quiet) for\n",
        "            idx, num_walks\n",
        "            in enumerate(num_walks_lists, 1))\n",
        "\n",
        "        walks = flatten(walk_results)\n",
        "\n",
        "        return walks\n",
        "\n",
        "    def fit(self, **skip_gram_params) -> gensim.models.Word2Vec:\n",
        "        \"\"\"\n",
        "        Creates the embeddings using gensim's Word2Vec.\n",
        "        :param skip_gram_params: Parameteres for gensim.models.Word2Vec - do not supply 'size' it is taken from the Node2Vec 'dimensions' parameter\n",
        "        :type skip_gram_params: dict\n",
        "        :return: A gensim word2vec model\n",
        "        \"\"\"\n",
        "\n",
        "        if 'workers' not in skip_gram_params:\n",
        "            skip_gram_params['workers'] = self.workers\n",
        "\n",
        "        if 'size' not in skip_gram_params:\n",
        "            skip_gram_params['size'] = self.dimensions\n",
        "\n",
        "        return gensim.models.Word2Vec(self.walks, **skip_gram_params)\n",
        "\n",
        "\n",
        "def parallel_generate_walks(d_graph: dict, global_walk_length: int, num_walks: int, cpu_num: int,\n",
        "                            sampling_strategy: dict = None, num_walks_key: str = None,\n",
        "                            walk_length_key: str = None, neighbors_key: str = None,\n",
        "                            probabilities_key: str = None, first_travel_key: str = None,\n",
        "                            quiet: bool = False) -> list:\n",
        "    \"\"\"\n",
        "    Generates the random walks which will be used as the skip-gram input.\n",
        "    :return: List of walks. Each walk is a list of nodes.\n",
        "    \"\"\"\n",
        "\n",
        "    walks = list()\n",
        "\n",
        "    # if not quiet:\n",
        "    #     pbar = tqdm(total = num_walks, desc='Generating walks (CPU: {})'.format(cpu_num))\n",
        "\n",
        "    for n_walk in range(num_walks):\n",
        "\n",
        "        # Update progress bar\n",
        "        # if not quiet:\n",
        "        #     pbar.update(1)\n",
        "\n",
        "        # Shuffle the nodes\n",
        "        shuffled_nodes = list(d_graph.keys())\n",
        "        random.shuffle(shuffled_nodes)\n",
        "\n",
        "        # Start a random walk from every node\n",
        "        for source in shuffled_nodes:\n",
        "\n",
        "            # Skip nodes with specific num_walks\n",
        "            if source in sampling_strategy and \\\n",
        "                    num_walks_key in sampling_strategy[source] and \\\n",
        "                    sampling_strategy[source][num_walks_key] <= n_walk:\n",
        "                continue\n",
        "\n",
        "            # Start walk\n",
        "            walk = [source]\n",
        "\n",
        "            # Calculate walk length\n",
        "            if source in sampling_strategy:\n",
        "                walk_length = sampling_strategy[source].get(walk_length_key, global_walk_length)\n",
        "            else:\n",
        "                walk_length = global_walk_length\n",
        "\n",
        "            # Perform walk\n",
        "            while len(walk) < walk_length:\n",
        "\n",
        "                walk_options = d_graph[walk[-1]].get(neighbors_key, None)\n",
        "\n",
        "                # Skip dead end nodes\n",
        "                if not walk_options:\n",
        "                    break\n",
        "\n",
        "                if len(walk) == 1:  # For the first step\n",
        "                    probabilities = d_graph[walk[-1]][first_travel_key]\n",
        "                    walk_to = np.random.choice(walk_options, size=1, p=probabilities)[0]\n",
        "                else:\n",
        "                    probabilities = d_graph[walk[-1]][probabilities_key][walk[-2]]\n",
        "                    walk_to = np.random.choice(walk_options, size=1, p=probabilities)[0]\n",
        "\n",
        "                walk.append(walk_to)\n",
        "\n",
        "            walk = list(map(str, walk))  # Convert all to strings\n",
        "\n",
        "            walks.append(walk)\n",
        "\n",
        "    # if not quiet:\n",
        "    #     pbar.close()\n",
        "\n",
        "    return walks\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    instantiate a node2vec object\n",
        "    \"\"\"\n",
        "    print(\"Node2Vec main method.\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # args = parse_args()\n",
        "\n",
        "    # main_path = args.main_path\n",
        "    # graph_name = args.graph #eth_0\n",
        "\n",
        "    # iter_num = args.iter    #5\n",
        "    # num_walks = args.num_walks  #20\n",
        "    # dim = args.dimensions   #64\n",
        "    # walk_length = args.walk_length  #5\n",
        "    # workers = args.workers  #10\n",
        "    # window_size = args.window_size  #10\n",
        "    # p = args.p  #1\n",
        "    # q = args.q  #1\n",
        "\n",
        "    iter_num = 5\n",
        "    num_walks = 20\n",
        "    dim = 64\n",
        "    walk_length = 5\n",
        "    workers = 10\n",
        "    window_size = 10\n",
        "    p = 1\n",
        "    q = 1\n",
        "    exp_id = '1;elliptic'\n",
        "\n",
        "    edge_list_file_name = '/content/drive/My Drive/BaselineToShow/edgeD1.csv'\n",
        "    node_list_file_name = '/content/drive/My Drive/BaselineToShow/nodeD1.csv'\n",
        "    stats_file = '/content/drive/My Drive/BaselineToShow/stats.csv'\n",
        "    embeddings_filename = '/content/drive/My Drive/BaselineToShow/embeddings.emb'\n",
        "\n",
        "    nx_g = nx.from_pandas_edgelist(pd.read_csv(edge_list_file_name), source='source', target='target', create_using=nx.DiGraph())\n",
        "    print(\"Graph info:\", nx.info(nx_g))\n",
        "    # for edge in nx_g.edges():\n",
        "    #     nx_g[edge[0]][edge[1]]['weight'] = 1\n",
        "    # nx_g = nx_g.to_undirected()\n",
        "\n",
        "    print(\"\\tInstantiate a node2vec object.\")\n",
        "    node2vec = Node2Vec(nx_g, dimensions=dim, walk_length=walk_length,\n",
        "                        num_walks=num_walks, workers=workers, p=p, q=q)\n",
        "    print(\"\\tFit node2vec.\")\n",
        "    model = node2vec.fit(window=window_size, sg=1, hs=0, min_count=1, iter=iter_num)\n",
        "\n",
        "    # read node list\n",
        "    print(\"\\tExtract embeddings and labels for the anchor nodes.\")\n",
        "    nodes_df = pd.read_csv(node_list_file_name)\n",
        "\n",
        "    # binary classification anchor nodes\n",
        "    anchor_nodes_df = nodes_df\n",
        "    node_list = [str(node_id) for node_id in anchor_nodes_df['node'].tolist()]\n",
        "    embeddings = [model.wv.get_vector(node) for node in node_list]\n",
        "    print(type(embeddings))\n",
        "    model.wv.save_word2vec_format(embeddings_filename)\n",
        "    print('Tanay')\n",
        "    labels = anchor_nodes_df['isp'].tolist()\n",
        "\n",
        "\n",
        "    # # classification\n",
        "    # print(\"\\tApply classification.\")\n",
        "    # rnd_seed = 42\n",
        "    # binary = True\n",
        "    # X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, rnd_seed)\n",
        "    # rf_lr_classification(X_train, X_test, y_train, y_test, stats_file, 'n2v',\n",
        "    #                                         binary, exp_id, print_report=True)\n",
        "    # print(\"Total elapsed time:\", str(time.time() - start_time))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}